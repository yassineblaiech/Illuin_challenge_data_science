{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abef2f36",
   "metadata": {},
   "source": [
    "### Installing packages on collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9722f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.2/517.2 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pandasql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.0)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "p7zip-full is already the newest version (16.02+dfsg-8).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q \"transformers[torch]\" trl datasets peft accelerate bitsandbytes pandasql python-Levenshtein\n",
    "!pip install -U bitsandbytes\n",
    "!pip install -U transformers accelerate peft\n",
    "!pip install -q gdown\n",
    "!apt-get install -y p7zip-full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fcb521",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8249755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Imports & global config\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "import ast\n",
    "import re\n",
    "import shutil\n",
    "import base64\n",
    "from IPython.display import HTML, display\n",
    "from datasets import Dataset\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import Levenshtein as lev\n",
    "import os\n",
    "\n",
    "#MODEL_ID = \"Qwen/Qwen2.5-Coder-7B\"\n",
    "MODEL_ID = \"Qwen/Qwen3-1.7B\"\n",
    "# Subset pour tester rapidement\n",
    "TRAIN_NUM_SHARDS = 10\n",
    "TRAIN_SHARD_INDEX = 0\n",
    "TEST_NUM_SHARDS = 5\n",
    "TEST_SHARD_INDEX = 0\n",
    "\n",
    "MAX_EVAL_SAMPLES = 200  # nb d'exemples d'évaluation\n",
    "DO_TRAIN = True\n",
    "      \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f506986",
   "metadata": {},
   "source": [
    "### Loading and formatting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a42f573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file with ID: 1x-6BoOuGfY3HQnpixDiu8wWQHWHwyrfi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1x-6BoOuGfY3HQnpixDiu8wWQHWHwyrfi\n",
      "To: /content/code_classification_dataset.zip\n",
      "100%|██████████| 3.66M/3.66M [00:00<00:00, 20.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded file size: 3572.02 KB\n",
      "Extracting 7z archive...\n",
      "\n",
      "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
      "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
      "\n",
      "Scanning the drive for archives:\n",
      "  0M Sca        1 file, 3657745 bytes (3573 KiB)\n",
      "\n",
      "Extracting archive: code_classification_dataset.zip\n",
      "WARNING:\n",
      "code_classification_dataset.zip\n",
      "Can not open the file as [zip] archive\n",
      "The file is open as [7z] archive\n",
      "\n",
      "--\n",
      "Path = code_classification_dataset.zip\n",
      "Open WARNING: Can not open the file as [zip] archive\n",
      "Type = 7z\n",
      "Physical Size = 3657745\n",
      "Headers Size = 42550\n",
      "Method = LZMA2:24\n",
      "Solid = +\n",
      "Blocks = 1\n",
      "\n",
      "      0% 849 - code_classification_dataset/sample_1761.jso                                                       26% 1240 - code_classification_dataset/sample_2112.js                                                       26% 1667 - code_classification_dataset/sample_2498.js                                                       26% 2015 - code_classification_dataset/sample_2810.js                                                       53% 2233 - code_classification_dataset/sample_3006.js                                                       53% 2502 - code_classification_dataset/sample_3249.js                                                       53% 2733 - code_classification_dataset/sample_3457.js                                                       53% 2964 - code_classification_dataset/sample_3665.js                                                       53% 3176 - code_classification_dataset/sample_3856.js                                                       60% 3334 - code_classification_dataset/sample_3999.js                                                       60% 3526 - code_classification_dataset/sample_4170.js                                                       60% 3699 - code_classification_dataset/sample_4326.js                                                       60% 3822 - code_classification_dataset/sample_4437.js                                                       60% 3932 - code_classification_dataset/sample_4536.js                                                       86% 4027 - code_classification_dataset/sample_4621.js                                                       86% 4129 - code_classification_dataset/sample_4713.js                                                       86% 4230 - code_classification_dataset/sample_4804.js                                                       86% 4328 - code_classification_dataset/sample_4893.js                                                       86% 4424 - code_classification_dataset/sample_498.jso                                                       86% 4520 - code_classification_dataset/sample_582.jso                                                       86% 4614 - code_classification_dataset/sample_667.jso                                                       86% 4702 - code_classification_dataset/sample_746.jso                                                       86% 4836 - code_classification_dataset/sample_867.jso                                                       86% 4965 - code_classification_dataset/sample_983.jso                                                      Everything is Ok\n",
      "\n",
      "Archives with Warnings: 1\n",
      "Folders: 1\n",
      "Files: 4982\n",
      "Size:       20685598\n",
      "Compressed: 3657745\n",
      "Distinct tag combinations found: 1907\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "import os\n",
    "import json\n",
    "import gdown\n",
    "\n",
    "def folder_to_dataframe(folder_path):\n",
    "    \"\"\"\n",
    "    Reads all JSON files from a specified folder and converts them into a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    folder_path (str): The path to the folder containing JSON files.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the combined data from all JSON files.\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Error: The folder '{folder_path}' does not exist.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    \n",
    "                    if isinstance(data, list):\n",
    "                        data_list.extend(data)\n",
    "                    else:\n",
    "                        data_list.append(data)\n",
    "                        \n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: Could not decode {filename}. Skipping.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(data_list)\n",
    "    return df\n",
    "\n",
    "def clean_format_dataframe(df):\n",
    "    \"\"\"\n",
    "    Formats the DataFrame by ensuring consistent column names, data types,\n",
    "    and one-hot encoding the tags column.\n",
    "    \"\"\"\n",
    "    columns_to_keep = ['tags', 'source_code', 'difficulty','prob_desc_input_spec','prob_desc_output_spec','prob_desc_description','prob_desc_notes']\n",
    "    df_cleaned = df[columns_to_keep].copy()\n",
    "    \n",
    "    distinct_tuples = df_cleaned['tags'].apply(tuple).unique()\n",
    "    distinct_lists = [list(x) for x in distinct_tuples]\n",
    "    print(f\"Distinct tag combinations found: {len(distinct_lists)}\")\n",
    "    \n",
    "    tags_to_keep = ['math', 'graphs', 'strings', 'number theory', 'trees', 'geometry', 'games', 'probabilities']\n",
    "    \n",
    "    def filter_tags(tag_list):\n",
    "        return [tag for tag in tag_list if tag in tags_to_keep]\n",
    "    \n",
    "    df_cleaned['tags'] = df_cleaned['tags'].apply(filter_tags)\n",
    "\n",
    "    for tag in tags_to_keep:\n",
    "        col_name = f\"tag_{tag.replace(' ', '_')}\" \n",
    "        df_cleaned[col_name] = df_cleaned['tags'].apply(lambda x: 1 if tag in x else 0)\n",
    "        \n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "### This part is dedicated to downloading and extracting the dataset\n",
    "### Uncomment and run this section if you need to download the dataset on collab extension in vscode\n",
    "\n",
    "file_id = '1x-6BoOuGfY3HQnpixDiu8wWQHWHwyrfi'\n",
    "output_file = 'code_classification_dataset.zip'\n",
    "\n",
    "print(f\"Downloading file with ID: {file_id}...\")\n",
    "gdown.download(id=file_id, output=output_file, quiet=False)\n",
    "\n",
    "file_size = os.path.getsize(output_file)\n",
    "print(f\"Downloaded file size: {file_size / 1024:.2f} KB\")\n",
    "\n",
    "filename = \"code_classification_dataset.zip\"\n",
    "\n",
    "print(\"Extracting 7z archive...\")\n",
    "!7z x code_classification_dataset.zip -o/content/code_classification_dataset/\n",
    "\n",
    "\n",
    "df = folder_to_dataframe(\"/content/code_classification_dataset/code_classification_dataset/\")\n",
    "df = clean_format_dataframe(df)\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6308962",
   "metadata": {},
   "source": [
    "### Spliting train, test and valiation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f8c2380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2491 (50.0%)\n",
      "Val size:   1245 (25.0%)\n",
      "Test size:  1246 (25.0%)\n"
     ]
    }
   ],
   "source": [
    "train_testvalid = dataset.train_test_split(test_size=0.5, seed=42)\n",
    "train_dataset = train_testvalid['train']\n",
    "test_valid_dataset = train_testvalid['test']\n",
    "\n",
    "valid_test = test_valid_dataset.train_test_split(test_size=0.5, seed=42)\n",
    "val_dataset = valid_test['train']\n",
    "test_dataset = valid_test['test']\n",
    "\n",
    "train_small = train_dataset\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)} ({len(train_dataset)/len(dataset):.1%})\")\n",
    "print(f\"Val size:   {len(val_dataset)} ({len(val_dataset)/len(dataset):.1%})\")\n",
    "print(f\"Test size:  {len(test_dataset)} ({len(test_dataset)/len(dataset):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028c8dd3",
   "metadata": {},
   "source": [
    "### Formating prompts for train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f250e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_train(example):\n",
    "    return f\"\"\"### INSTRUCTION\n",
    "Given the following problem description, classify it to the following categories:['math', 'graphs', 'strings', 'number theory', 'trees', 'geometry', 'games', 'probabilities'].\n",
    "\n",
    "\n",
    "### PROB DESCRIPTION NOTES\n",
    "{example['prob_desc_notes']}\n",
    "\n",
    "### PROB DESCRIPTION OUTPUT SPEC\n",
    "{example['prob_desc_output_spec']}\n",
    "\n",
    "### PROB DESCRIPTION INPUT SPEC\n",
    "{example['prob_desc_input_spec']}\n",
    "\n",
    "### PROB DESCRIPTION \n",
    "{example['prob_desc_description']}\n",
    "\n",
    "### CATEGORY\n",
    "{example['tags']}\n",
    "\"\"\"\n",
    "\n",
    "def format_prompt_eval(example):\n",
    "    return f\"\"\"### INSTRUCTION\n",
    "Given the following problem description, classify it to the following categories:['math', 'graphs', 'strings', 'number theory', 'trees', 'geometry', 'games', 'probabilities'].\n",
    "\n",
    "\n",
    "### PROB DESCRIPTION NOTES\n",
    "{example['prob_desc_notes']}\n",
    "\n",
    "### PROB DESCRIPTION OUTPUT SPEC\n",
    "{example['prob_desc_output_spec']}\n",
    "\n",
    "### PROB DESCRIPTION INPUT SPEC\n",
    "{example['prob_desc_input_spec']}\n",
    "\n",
    "### PROB DESCRIPTION \n",
    "{example['prob_desc_description']}\n",
    "\n",
    "### CATEGORY\n",
    "[\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c4f3715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_eval_prompt(row):\n",
    "    return format_prompt_eval(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe231d6",
   "metadata": {},
   "source": [
    "### Loading the llm model and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "254c8d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15103c15a1ff4c71a198fb5cced98e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f549f3bcfe5242f1abd01ba8e2ad4ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0292da29d7c740b1a1e6803f0ef6a1fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0b2c340f9f4789a7af4dcbe9afd9a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b0270afd5a406bb8b9d4ed232a207f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a136f28907f8474eb583c0d12318f2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b67d0315034af89711d449d7824676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b066042e633a4141b17510041d302283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb2da79995f4a8cacd8d98e569495c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/622M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a20c0abd6d4d949e1e8272f4894167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca6c88fd50d4d88b18707cd0ba0fc8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.model_max_length = 2048\n",
    "tokenizer.truncation_side = \"left\"\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,     \n",
    "    device_map=\"auto\",          \n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "base_model.gradient_checkpointing_enable()\n",
    "base_model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060e3dda",
   "metadata": {},
   "source": [
    "### Lora configuration and setup of training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1286a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sql_finetune_results\",\n",
    "    per_device_train_batch_size=2,     \n",
    "    gradient_accumulation_steps=4,     \n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,               \n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bef352",
   "metadata": {},
   "source": [
    "### Finetuning launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0011d2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4003d64dbbec4122b17a03692151fb8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/2491 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6fee93a96742f1acca519b3845ac85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/2491 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0d598758674d78985ad65d2091da64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/2491 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8862b10457224650866cdecd9fae9584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2491 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Start training ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1373051068.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=== Start training ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0madapter_save_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./llm_challenge_adapter\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_activation_offload_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1245\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4069\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4071\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2850\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2851\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2852\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_small,\n",
    "    formatting_func=format_prompt_train,\n",
    "    peft_config=peft_config,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "\n",
    "if DO_TRAIN:\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"=== Start training ===\")\n",
    "    trainer.train()\n",
    "    adapter_save_path = \"./llm_challenge_adapter\"\n",
    "    trainer.save_model(adapter_save_path)\n",
    "    print(\"LoRA adapter saved to\", adapter_save_path)\n",
    "else:\n",
    "    adapter_save_path = \"./llm_challenge_adapter\"\n",
    "    print(\"Training skipped, expecting existing adapter at\", adapter_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67922ed4",
   "metadata": {},
   "source": [
    "### Save and download the model after fine tuning from colab (do not launch if you didn't train the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f93c5938",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adapter_save_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-165573485.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdownload_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'adapter_save_path' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def download_directory(path, filename=None):\n",
    "    \"\"\"\n",
    "    Zips a directory and creates a clickable download link for the zip file.\n",
    "    \n",
    "    path: full path to the directory (e.g. '/content/my_model')\n",
    "    filename: optional name for the output zip (without .zip extension)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Path not found: {path}\")\n",
    "\n",
    "    folder_name = os.path.basename(path.rstrip(os.sep))\n",
    "    filename = filename or folder_name\n",
    "\n",
    "    print(f\"Zipping directory '{path}'...\")\n",
    "    zip_path = shutil.make_archive(filename, 'zip', path)\n",
    "    \n",
    "    with open(zip_path, \"rb\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    b64 = base64.b64encode(data).decode(\"utf-8\")\n",
    "    download_filename = os.path.basename(zip_path)\n",
    "    href = f'data:application/zip;base64,{b64}'\n",
    "    html = f'<a download=\"{download_filename}\" href=\"{href}\">Download {download_filename}</a>'\n",
    "\n",
    "    display(HTML(html))\n",
    "    \n",
    "\n",
    "download_directory(adapter_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad178c",
   "metadata": {},
   "source": [
    "### reload the model for evaluation (avoid retraining the model by downloading weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eba62788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file with ID: 1fEvoMixqRp3svBscQWCWrVQBU-UACmlw...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1fEvoMixqRp3svBscQWCWrVQBU-UACmlw\n",
      "To: /content/llm_challenge_adapter.zip\n",
      "100%|██████████| 15.8M/15.8M [00:00<00:00, 49.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded file size: 15436.95 KB\n",
      "Extracting 7z archive...\n",
      "\n",
      "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
      "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
      "\n",
      "Scanning the drive for archives:\n",
      "  0M Sca        1 file, 15807440 bytes (16 MiB)\n",
      "\n",
      "Extracting archive: llm_challenge_adapter.zip\n",
      "--\n",
      "Path = llm_challenge_adapter.zip\n",
      "Type = zip\n",
      "Physical Size = 15807440\n",
      "\n",
      "     90% 10 - vocab.js                  Everything is Ok\n",
      "\n",
      "Files: 11\n",
      "Size:       28754604\n",
      "Compressed: 15807440\n",
      "Extracted adapter to: ./llm_challenge_adapter\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### This part is dedicated to downloading and extracting the model\n",
    "### Uncomment and run this section if you need to download the model on collab extension in vscode\n",
    "\n",
    "file_id = '1fEvoMixqRp3svBscQWCWrVQBU-UACmlw'\n",
    "output_file = 'llm_challenge_adapter.zip'\n",
    "\n",
    "print(f\"Downloading file with ID: {file_id}...\")\n",
    "gdown.download(id=file_id, output=output_file, quiet=False)\n",
    "\n",
    "file_size = os.path.getsize(output_file)\n",
    "print(f\"Downloaded file size: {file_size / 1024:.2f} KB\")\n",
    "\n",
    "filename = \"llm_challenge_adapter.zip\"\n",
    "\n",
    "print(\"Extracting 7z archive...\")\n",
    "!7z x llm_challenge_adapter.zip -o/content/llm_challenge_adapter/\n",
    "\n",
    "zip_file_path = \"llm_challenge_adapter.zip\" \n",
    "extract_path = \"./llm_challenge_adapter\"\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(f\"Extracted adapter to: {extract_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6ab23c",
   "metadata": {},
   "source": [
    "### Initializing the finetuned LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65657866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11bce50822f746eda83b0f9e13255ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading adapter from ./llm_challenge_adapter...\n",
      "Adapter loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen3ForCausalLM(\n",
       "      (model): Qwen3Model(\n",
       "        (embed_tokens): Embedding(151936, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "              (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "        (rotary_emb): Qwen3RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ADAPTER_PATH = \"./llm_challenge_adapter\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" \n",
    "\n",
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"Loading adapter from {ADAPTER_PATH}...\")\n",
    "try:\n",
    "    model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "    print(\"Adapter loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading adapter. Check if path exists and contains adapter_config.json.\\nError: {e}\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8681bf",
   "metadata": {},
   "source": [
    "### Evaluation setup and metrics definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ffb1d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['math', 'graphs', 'strings', 'number theory', 'trees', 'geometry', 'games', 'probabilities']\n",
    "\n",
    "def robust_extract_tags(text):\n",
    "    \"\"\"\n",
    "    Scans the text for the presence of valid labels.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    found_tags = []\n",
    "    \n",
    "    # Use the global LABELS variable\n",
    "    for label in LABELS:\n",
    "        if label in text:\n",
    "            found_tags.append(label)\n",
    "            \n",
    "    return found_tags\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, num_samples=None):\n",
    "    model.eval()\n",
    "    \n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    eval_data = dataset.select(range(num_samples)) if num_samples else dataset\n",
    "\n",
    "    print(f\"Starting evaluation on {len(eval_data)} samples...\")\n",
    "\n",
    "    for i, row in tqdm(enumerate(eval_data), total=len(eval_data)):\n",
    "        prompt = format_prompt_eval(row)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=50, \n",
    "                do_sample=False, \n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        answer_only = output_text[len(prompt):]\n",
    "        \n",
    "        prediction = robust_extract_tags(answer_only)\n",
    "        \n",
    "        truth = row['tags'] \n",
    "        \n",
    "        pred_labels.append(prediction)\n",
    "        true_labels.append(truth)\n",
    "\n",
    "    return true_labels, pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaec1b6e",
   "metadata": {},
   "source": [
    "### Example of an output of the finetuned LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa4532d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GROUND TRUTH ===\n",
      "[]\n",
      "\n",
      "=== RAW MODEL ANSWER (After Prompt) ===\n",
      "' ]\n",
      "'\n",
      "\n",
      "=== FULL CONTEXT (Prompt + Answer) ===\n",
      "### INSTRUCTION\n",
      "Given the following problem description, classify it to the following categories:['math', 'graphs', 'strings', 'number theory', 'trees', 'geometry', 'games', 'probabilities'].\n",
      "\n",
      "\n",
      "### PROB DESCRIPTION NOTES\n",
      "NoteIn the first test case,   The substring is $$$\\texttt{101}$$$, so we can do one operation to make the substring empty.  The substring is $$$\\texttt{11011}$$$, so we can do one operation on $$$s[2, 4]$$$ to make $$$\\texttt{11}$$$, then use two more operations to make the substring empty.  The substring is $$$\\texttt{011}$$$, so we can do one operation on $$$s[1, 2]$$$ to make $$$\\texttt{1}$$$, then use one more operation to make the substring empty. \n",
      "\n",
      "### PROB DESCRIPTION OUTPUT SPEC\n",
      "Print $$$q$$$ lines, the $$$i$$$-th line representing the minimum number of operations needed for the $$$i$$$-th query.\n",
      "\n",
      "### PROB DESCRIPTION INPUT SPEC\n",
      "The first line contains two integers $$$n$$$ and $$$q$$$ ($$$1 \\le n, q \\le 2 \\cdot 10 ^ 5$$$)  — the length of the binary string $$$a$$$ and the number of queries respectively. The second line contains a binary string $$$a$$$ of length $$$n$$$ ($$$a_i \\in \\{0, 1\\}$$$). Each of the next $$$q$$$ lines contains two integers $$$l$$$ and $$$r$$$ ($$$1 \\le l \\le r \\le n$$$)  — representing the substring of each query.\n",
      "\n",
      "### PROB DESCRIPTION \n",
      "You have a binary string $$$a$$$ of length $$$n$$$ consisting only of digits $$$0$$$ and $$$1$$$. You are given $$$q$$$ queries. In the $$$i$$$-th query, you are given two indices $$$l$$$ and $$$r$$$ such that $$$1 \\le l \\le r \\le n$$$. Let $$$s=a[l,r]$$$. You are allowed to do the following operation on $$$s$$$:  Choose two indices $$$x$$$ and $$$y$$$ such that $$$1 \\le x \\le y \\le |s|$$$. Let $$$t$$$ be the substring $$$t = s[x, y]$$$. Then for all $$$1 \\le i \\le |t| - 1$$$, the condition $$$t_i \\neq t_{i+1}$$$ has to hold. Note that $$$x = y$$$ is always a valid substring.  Delete the substring $$$s[x, y]$$$ from $$$s$$$. For each of the $$$q$$$ queries, find the minimum number of operations needed to make $$$s$$$ an empty string.Note that for a string $$$s$$$, $$$s[l,r]$$$ denotes the subsegment $$$s_l,s_{l+1},\\ldots,s_r$$$.\n",
      "\n",
      "### CATEGORY\n",
      "[ ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_index = 5\n",
    "example = test_dataset[example_index]\n",
    "\n",
    "prompt = format_prompt_eval(example)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=100,      \n",
    "        do_sample=False,         \n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "answer_start_index = len(prompt)\n",
    "raw_answer = full_output[answer_start_index:] \n",
    "\n",
    "print(f\"=== GROUND TRUTH ===\")\n",
    "print(example['tags'])\n",
    "\n",
    "print(f\"\\n=== RAW MODEL ANSWER (After Prompt) ===\")\n",
    "print(f\"'{raw_answer}'\")  \n",
    "\n",
    "print(f\"\\n=== FULL CONTEXT (Prompt + Answer) ===\")\n",
    "print(full_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b6ca80",
   "metadata": {},
   "source": [
    "### Evaluate the llm with finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01d4f251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on 1200 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [08:34<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Results ===\n",
      "Micro F1 Score: 0.2442\n",
      "Macro F1 Score: 0.1183\n",
      "Precision:      0.3763\n",
      "Recall:         0.1808\n",
      "\n",
      "=== Detailed Classification Report ===\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         math       0.41      0.37      0.39       328\n",
      "       graphs       0.22      0.03      0.05       137\n",
      "      strings       0.53      0.11      0.18        84\n",
      "number theory       0.12      0.01      0.02        83\n",
      "        trees       0.40      0.03      0.05        77\n",
      "     geometry       0.12      0.05      0.08        37\n",
      "        games       1.00      0.04      0.07        26\n",
      "probabilities       0.11      0.11      0.11        19\n",
      "\n",
      "    micro avg       0.38      0.18      0.24       791\n",
      "    macro avg       0.36      0.09      0.12       791\n",
      " weighted avg       0.36      0.18      0.21       791\n",
      "  samples avg       0.11      0.10      0.10       791\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true_lists, y_pred_lists = evaluate_model(base_model, tokenizer, test_dataset, num_samples=1200)\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=LABELS)\n",
    "y_true_bin = mlb.fit_transform(y_true_lists)\n",
    "y_pred_bin = mlb.transform(y_pred_lists)\n",
    "\n",
    "micro_f1 = f1_score(y_true_bin, y_pred_bin, average='micro')\n",
    "macro_f1 = f1_score(y_true_bin, y_pred_bin, average='macro')\n",
    "precision = precision_score(y_true_bin, y_pred_bin, average='micro')\n",
    "recall = recall_score(y_true_bin, y_pred_bin, average='micro')\n",
    "\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(f\"Micro F1 Score: {micro_f1:.4f}\")\n",
    "print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "print(f\"Precision:      {precision:.4f}\")\n",
    "print(f\"Recall:         {recall:.4f}\")\n",
    "\n",
    "print(\"\\n=== Detailed Classification Report ===\")\n",
    "print(classification_report(y_true_bin, y_pred_bin, target_names=LABELS, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a617bed",
   "metadata": {},
   "source": [
    "### Evaluate the llm without finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18071fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on 1200 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [52:58<00:00,  2.65s/it]\n"
     ]
    }
   ],
   "source": [
    "with model.disable_adapter():\n",
    "    y_true_lists_base, y_pred_lists_base = evaluate_model(model, tokenizer, test_dataset, num_samples=1200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79f36436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Results ===\n",
      "Micro F1 Score: 0.1473\n",
      "Macro F1 Score: 0.1069\n",
      "Precision:      0.1409\n",
      "Recall:         0.1542\n",
      "\n",
      "=== Detailed Classification Report ===\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         math       0.39      0.11      0.17       328\n",
      "       graphs       0.11      0.38      0.18       137\n",
      "      strings       0.36      0.30      0.33        84\n",
      "number theory       0.06      0.04      0.05        83\n",
      "        trees       0.16      0.06      0.09        77\n",
      "     geometry       0.04      0.03      0.03        37\n",
      "        games       0.01      0.04      0.01        26\n",
      "probabilities       0.00      0.00      0.00        19\n",
      "\n",
      "    micro avg       0.14      0.15      0.15       791\n",
      "    macro avg       0.14      0.12      0.11       791\n",
      " weighted avg       0.24      0.15      0.15       791\n",
      "  samples avg       0.08      0.08      0.07       791\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer(classes=LABELS)\n",
    "y_true_bin = mlb.fit_transform(y_true_lists_base)\n",
    "y_pred_bin = mlb.transform(y_pred_lists_base)\n",
    "\n",
    "micro_f1 = f1_score(y_true_bin, y_pred_bin, average='micro')\n",
    "macro_f1 = f1_score(y_true_bin, y_pred_bin, average='macro')\n",
    "precision = precision_score(y_true_bin, y_pred_bin, average='micro')\n",
    "recall = recall_score(y_true_bin, y_pred_bin, average='micro')\n",
    "\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(f\"Micro F1 Score: {micro_f1:.4f}\")\n",
    "print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "print(f\"Precision:      {precision:.4f}\")\n",
    "print(f\"Recall:         {recall:.4f}\")\n",
    "\n",
    "print(\"\\n=== Detailed Classification Report ===\")\n",
    "print(classification_report(y_true_bin, y_pred_bin, target_names=LABELS, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
